---
title: 计算机操作系统（二）：物理内存
type: tags
tags:
  - null
date: 2019-02-28 16:37:10
categories:
description:
---

# 物理内存管理

## 计算机体系结构和内存层次

### 计算机体系结构

计算机系统32位，即地址总线是32位的。

CPU寄存器（32位、64位等）、内存、外存

CPU：

![1551415868764](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551415868764.png)

### 内存层次

CPU：两级缓存，由硬件MMU控制，3.6GHZ(CPU的频率)

内存（高速缓存未命中），由OS控制，1.3GHZ

外存（缺页）5ms

### OS的内存管理方式

OS内存管理需要实现的目标

- 抽象，将物料地址空间转换为逻辑地址空间
- 保护，对地址空间进行保护，独立地址空间
- 共享，访问相同内存
- 虚拟化，更大的地址空间

对于内存的访问，以字节进行访问，每个字节有一个物理地址

对于外存的访问，

**内存管理方式**

- 重定位，将每一个地址用一个段地址与一个偏移，可以使得程序可移植
- 分段，使得地址空间不必连续，但是一段必须连续
- 分页，将内存分为最基本的单位，
- 虚拟内存

# 地址空间&地址生成

## **地址空间的定义**

- 物理地址空间，硬件支持的地址空间
  - 起始地址到MAX内存
- 逻辑地址空间，在CPU运行的进程看到的地址
  - 起始地址0，到MAXprog

## **地址生成**

逻辑地址的生成

![1551416925221](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551416925221.png)

地址生成时机和限制

- 编译时
  - 假设起始地址已知，如果起始地址改变，必须重新编译
- 加载时（加载之后，地址就无法改变了）
  - 编译时起始地址未知，进行重定位生成绝对地址
- 执行时（不要求地址空间移动）
  - 执行时代码可移动
  - 需地址转换（虚拟内存）硬件支持

## **地址生成过程**

![1551418025948](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551418025948.png)

## **地址检测**

![1551418392013](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551418392013.png)

# 连续内存分配

在没有其他技术支持的情况下，分配的内存必须是连续的。为提高利用内存的利用效率，从如何找需要的内存分区与如何处理不能用的内存分区来考虑。

连续内存分配：给进程分配一块不小于指定大小的连续的物理内存区域

## 内存碎片

内存碎片：空闲内存不能被利用

- 外部碎片：分配单元之间未被使用内存
- 内部碎片：分配单元内部的未被使用内存（需要511字节，但是只能以512字节分配），取决于分配单元大小是否取整

## 动态分配

当程序被加载进行时，分配一个进程指定大小可变的分区

分区的地址是连续的

**操作系统需要维护的数据结构**

- 所有进程的已分配分区
- 空闲分区

**分配策略**

- 最先匹配
- 最佳匹配，将所有的空闲分区查找一遍
- 最差匹配，使用最大的

## 碎片整理

通过碎片整理获得更大的内存空间。通过调整进程占用的分区位置来减少或避免分区碎片

- 紧凑
  - 通过移动分配给进程的内存分区，以合并外部碎片
  - 条件：所有应用程序可动态重定位
  - 通常在进程等待时刻进行紧凑
  - 存在开销

- 分区对换
  - 通过抢占并回收处于等待状态进程的分区，以增大可用内存空间。
  - 开销相对较大，因为在外存的读取较慢

## 伙伴系统

连续内存分配的实例

- 整个可分配分区大小为2^n
- 将分区切半直到大于所需的内存空间

# 非连续内存分配

## 需求背景

如果用户不存在需要的连续的内存空间，分配就会失败

段式分配较大，

页式分配较小，分的太小，则逻辑地址到物理地址的对应关系就会有些复杂（处理方式：页表）

**连续分配**的缺点：

- 分配给程序的物理内存必须连续
- 存在外碎片与内碎片
- 内存分配的动态修改困难
- 内存利用率较低

**非连续分配**

设计目标：提高内存利用效率与管理灵活性

- 允许一个程序的使用非连续的物理地址空间
- 允许共享代码和数据
- 支持动态加载与动态链接

需解决的问题

- 如何实现虚拟地址和物理地址的转换
  - 硬件实现（**够用**，开销小）
  - 软件实现（灵活，开销大，类似于外排序）
- 非连续分配的硬件辅助机制，如何选择非连续分配中的内存分块大小
  - 段式
  - 页式

## 段式存储管理

**段地址空间**

目的：更精细力度和灵活的分离与共享

进程的段地址空间由多个段组成

![1551443700539](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551443700539.png)

**段的概念**

- 段表示访问方式和存储数据等熟悉相同的一端地址空间
- 对应一个连续的内存块
- 若干个段组成进程逻辑地址空间

**段访问机制**

逻辑地址由二元组（s,addr）标识

- s:段号（查询段表，由OS控制），addr:段内偏移

## 页式存储管理

**概念**

- 页帧（帧，描述物理页面）
  - 把物理地址空间划分为大小相同的基本分配单位
  - 2^N
  - 内存物理地址表示：二元组（f,o）帧号，帧内偏移（值为s，表示每帧的字节为2^s）
- 页面（页，逻辑页面）
  - 把逻辑地址空间划分为大小相同的基本分配单位
  - 帧与页的大小相同

**地址转换**

页面到页帧

- 页表
  - 保存了逻辑地址-物理地址间的映射关系
- MMU/TLB（存储管理单元/快表，保证转换的高速进行）

逻辑地址的页号是连续的，但是物理地址中的帧号是不连续的

不是所有的页都有对应的帧

## 页表

**概述**

页表结构：

- 每个进程都有一个页表
- 每个页面对应一个页表项
  - 页表项组成：
  - 帧号
  - 页表项标志：存在位（如果逻辑页号有一个物理帧对应，则值为1）、修改位（内容是否修改）、引用位（是否在过去一段时间访问过）
- 页表内容会随着进程运行状态而动态变化
- 存放在页表基址寄存器PTBR

性能问题

- 访问一个内存单元需要两次内存访问
  - 第一次：读页表项
  - 第二次：访问数据
- 页表大小问题
  - 页表可能非常大
  - 64位机器里，页表大小非常大。**因为地址总线一共有64位，即想获得一个地址需要64位，地址总线当中，前a位为页表项，后面的64-a位为偏移量**

解决办法

- 缓存（快表，时间与空间的相邻性）
- 间接访问（即多级页表）

**快表**

概念

- 缓存近期访问的页表项
  - 在CPU里面，TLB使用关联存储实现，具备快速访问性能（但是CPU里面区域较小）
  - 如果TLB命中，则物理页号可以很快被获取
  - 未命中，对应表项更新到TLB当中

**多级页表**

概念

- 通过间接引用将页号分为k级
  - 建立页表树，整个访问次数为k+1
  - 减少每级页表长度
- 在大地址空间中很繁琐，逻辑地址空间（随着进程增多而增多）增长速度很快

**反置页表**

反置页表与页寄存器的解决办法

- 不让页表与逻辑地址空间的大小相对应（因此随着进程增多而不会增大）
- 让页表与物理地址空间的大小相对应

**页寄存器**

- 每个帧与一个页寄存器关联，寄存器内容
  - 使用位：是否被进程占用
  - 占用页号：对应页号p
  - 保护位

优点：

- 页表大小相对于物理内存很小
- 页表大小与逻辑地址空间大小无关

缺点

- 页表信息对调后，需要依靠帧号可找页号
- 在页寄存器中搜索逻辑地址中的页号（较困难）

地址转换

- CPU生产的逻辑地址如何找到对应物理地址
  - 对逻辑地址进行hash，减少搜索
- 用快表缓存页表项后的页寄存器搜索步骤
  - 对逻辑地址进行hash查找
  - 在块表中查找对应页表项
  - 查找失败时，产生异常
- 块表功耗等

**反置页表**

- 基于hash映射值查找对应页表项中的帧号（将进程标识加入一起进行hash，因为逻辑地址是与进程有关的，所以效率会高一点）
  - hash结束的结果以页帧号进行排序

# 段页式存储管理

段式存储在内存保护方面有优势，页式存储在内存利用和优化转移到后背存储方面有优势

**实现**

- 在段式存储管理基础上，给每个段加一个一级页表

![1551448914482](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551448914482.png)

**内存共享**

- 通过指向相同的页表基址，实现进程间的段共享

![1551448957878](C:\Users\Heper\AppData\Roaming\Typora\typora-user-images\1551448957878.png)

# 参考 #

1. 
